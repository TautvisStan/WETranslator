{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG1WVxKY-xYk"
      },
      "source": [
        "Pagal: https://keras.io/examples/nlp/neural_machine_translation_with_transformer/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W44SJQTH-z0-"
      },
      "source": [
        "Bibliotekų įdiegimas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BxUEvrj2-aY",
        "outputId": "10185725-cc65-4251-884d-409e59f6c555"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.1/792.1 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "keras-nlp 0.18.1 requires keras-hub==0.18.1, but you have keras-hub 0.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "keras-nlp 0.18.1 requires keras-hub==0.18.1, but you have keras-hub 0.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade rouge-score nltk\n",
        "!pip install -q --upgrade keras-hub\n",
        "!pip install -q --upgrade keras\n",
        "\n",
        "import keras_hub\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3hZxeMlkfHt"
      },
      "source": [
        "Bibliotekų importavimas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqHZ-3tKkfHt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import pathlib\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow.data as tf_data\n",
        "import tensorflow.strings as tf_strings\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import ops\n",
        "from keras.layers import TextVectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tKHxwiI-_UD"
      },
      "source": [
        "Duomenų failai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AY3PSOTckfHw"
      },
      "outputs": [],
      "source": [
        "text_file_en = \"/content/QED.en-lt.en\"\n",
        "text_file_lt = \"/content/QED.en-lt.lt\"\n",
        "text_file = \"/content/spa.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg-yr9C1kfHy"
      },
      "source": [
        "Duomenų paruošimas. Į vertimo eilutės pradžią pridedame \"[start]\", o į pabaigą \"[end]\". Viršuje kodas skirtas lietuviškam rinkiniui, apačioje - ispaniškui."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i18y1FoZkfHz"
      },
      "outputs": [],
      "source": [
        "# with open(text_file_en) as f:\n",
        "#     lines_en = f.read().split(\"\\n\")[:-1]\n",
        "# with open(text_file_lt) as f:\n",
        "#     lines_lt = f.read().split(\"\\n\")[:-1]\n",
        "# text_pairs = []\n",
        "# for line_en, line_lt in zip(lines_en, lines_lt):\n",
        "#     line_lt = \"[start] \" + line_lt + \" [end]\"\n",
        "#     text_pairs.append((line_en, line_lt))\n",
        "\n",
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    eng, tgt = line.split(\"\\t\")\n",
        "    tgt = \"[start] \" + tgt + \" [end]\"\n",
        "    text_pairs.append((eng, tgt))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Oa0XaG2kfH1"
      },
      "source": [
        "Pažiūrime kelias poras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3e0rdK3kfH3",
        "outputId": "1de37b9e-1859-42bd-cb14-884871456a05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('She suddenly became famous.', '[start] Ella de repente se hizo famosa. [end]')\n",
            "(\"Someone must've left the water running.\", '[start] Alguien tiene que haberse dejado el grifo abierto. [end]')\n",
            "('He fell in love with her at first sight.', '[start] Él se enamoró de ella a primera vista. [end]')\n",
            "(\"Tom can't read all these books in one day.\", '[start] Tom no puede leer todos estos libros en un día. [end]')\n",
            "('I had a lovely night.', '[start] Pasé una noche agradable. [end]')\n"
          ]
        }
      ],
      "source": [
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st8RJ0qekfH4"
      },
      "source": [
        "Išskiriame į treniravimo, validacijos ir testų poras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXQShXStkfH5",
        "outputId": "1e3b898c-ca4f-409d-c733-28f7210be61d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "118964 total pairs\n",
            "83276 training pairs\n",
            "17844 validation pairs\n",
            "17844 test pairs\n"
          ]
        }
      ],
      "source": [
        "random.seed(42)\n",
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipi7MN9N_ldt"
      },
      "source": [
        "Apsirašome hyperparametrus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7LikZU-j2Ny"
      },
      "outputs": [],
      "source": [
        "vocab_size = 50000\n",
        "sequence_length = 30\n",
        "epochs = 5\n",
        "embed_dim = 512\n",
        "latent_dim = 1024\n",
        "num_heads = 8\n",
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lV-dHu0_3rQ"
      },
      "source": [
        "Atliekama teksto vektorizacija paversti žodžius į skaičius."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfHTHTPSkfH6"
      },
      "outputs": [],
      "source": [
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf_strings.lower(input_string)\n",
        "    return tf_strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "\n",
        "eng_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "tgt_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_eng_texts = [pair[0] for pair in train_pairs]\n",
        "train_tgt_texts = [pair[1] for pair in train_pairs]\n",
        "eng_vectorization.adapt(train_eng_texts)\n",
        "tgt_vectorization.adapt(train_tgt_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xvW_eI6AHiX"
      },
      "source": [
        "Suformatuojame duomenų rinkinius. \"format_dataset\" - pritaiko vektorizaciją ir grąžina \"encoder_inputs\" įvestį encoderiui, \"decoder_inputs\" - tikslo sakinys be paskutinio žodžio (kad prognozuotų sekantį), \"tgt[:, 1:]\" - tikslai, kurios bandys prognozuoti.\n",
        "\n",
        "\"make_dataset\" - sukuria TensorFlow Dataset objektą iš sąrašų."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iwi5ARBxkfH7"
      },
      "outputs": [],
      "source": [
        "\n",
        "def format_dataset(eng, tgt):\n",
        "    eng = eng_vectorization(eng)\n",
        "    tgt = tgt_vectorization(tgt)\n",
        "    return (\n",
        "        {\n",
        "            \"encoder_inputs\": eng,\n",
        "            \"decoder_inputs\": tgt[:, :-1],\n",
        "        },\n",
        "        tgt[:, 1:],\n",
        "    )\n",
        "\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, tgt_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    tgt_texts = list(tgt_texts)\n",
        "    dataset = tf_data.Dataset.from_tensor_slices((eng_texts, tgt_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    return dataset.cache().shuffle(2048).prefetch(16)\n",
        "\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fy4_6gxkfH7"
      },
      "source": [
        "Pažiūrime į gautus shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xNoSmuHkfH8",
        "outputId": "725ac356-cbf9-4778-9b83-6553d45e4871"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (16, 30)\n",
            "inputs[\"decoder_inputs\"].shape: (16, 30)\n",
            "targets.shape: (16, 30)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OnL2dIyYZca"
      },
      "source": [
        "TransformerEncoder klasė:\n",
        "\n",
        "embed_dim: Nurodo vektorių dydį, kuriuo žodžiai yra reprezentuojami.\n",
        "\n",
        "dense_dim: dense sluoksnio dimensija.\n",
        "\n",
        "num_heads: multi-head attention mechanizmo galvų skaičius.\n",
        "attention = layers.MultiHeadAttention(...): Inicijuoja kelių galvų attention sluoksnį. Šis sluoksnis leidžia modeliui vienu metu atkreipti dėmesį į skirtingas įvesties sekos dalis. key_dim=embed_dim nurodo keys and queries dimensiją.\n",
        "\n",
        "dense_proj = keras.Sequential(...): Inicijuoja dense su ReLU aktyvacija. Šis sluoksnis apdoroja attention sluoksnio išvestį.\n",
        "layernorm_1 = layers.LayerNormalization() ir layernorm_2 = layers.LayerNormalization(): Inicijuoja normalizavimo sluoksnius.\n",
        "\n",
        "supports_masking = True: Nurodo, kad šis sluoksnis palaiko maskavimą, kas yra svarbu tvarkant kintamo ilgio sekas, kad būtų ignoruojama padding.\n",
        "\n",
        "call(self, inputs, mask=None): apibrėžia, kaip duomenys apdorojami per enkoderio sluoksnį.\n",
        "\n",
        "if mask is not None: padding_mask = ops.cast(mask[:, None, :], dtype=\"int32\"): pritaiko masking\n",
        "\n",
        "attention_output = self.attention(...): Apskaičiuojama attention išvestis.        proj_input = self.layernorm_1(inputs + attention_output): Prie įvesčių pridedama attention ir atliekamas normalizavimas.\n",
        "\n",
        "proj_output = self.dense_proj(proj_input): dense sluoksniai apdoroja normalizuotą išvestį.\n",
        "\n",
        "return self.layernorm_2(proj_input + proj_output): Prie normalizuotos įvesties pridedama dense išvestis ir atliekamas galutinis normalizavimas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WAephjrCdw3"
      },
      "source": [
        "---------------------\n",
        "\n",
        "PositionalEmbedding klasė:\n",
        "\n",
        "sequence_length: Maksimalus sekos ilgis.\n",
        "\n",
        "vocab_size: Žodyno dydis.\n",
        "\n",
        "embed_dim: embedding dimensija.\n",
        "\n",
        "token_embeddings = layers.Embedding(...): Konvertuoja žodžių indeksus į dense vektorius.\n",
        "\n",
        "position_embeddings = layers.Embedding(...): Inicijuoja embedding sluoksnį pozicijoms. Konvertuoja pozicijos indeksus (0, 1, 2, ...) į dense vektorius.\n",
        "\n",
        "call(self, inputs): apibrėžia, kaip apskaičiuojami poziciniai embedding ir kaip jie sujungiami su žodžių embedding.\n",
        "\n",
        "length = ops.shape(inputs)[-1]: Gaunamas dabartinės įvesties sekos ilgis.\n",
        "\n",
        "positions = ops.arange(0, length, 1): Sukuriama pozicijų seka (0, 1, 2, ..., ilgis-1).\n",
        "\n",
        "embedded_tokens = self.token_embeddings(inputs): Apskaičiuojami žodžių embedding.\n",
        "\n",
        "embedded_positions = self.position_embeddings(positions): Apskaičiuojami poziciniai embedding.\n",
        "\n",
        "return embedded_tokens + embedded_positions: Prie žodžių embedding pridedami poziciniai embedding. Šis sumavimas perduoda informaciją apie kiekvieno žodžio poziciją sekoje.\n",
        "\n",
        "compute_mask(self, inputs, mask=None): Šis metodas sukuria mask, kuri nurodo, kurie elementai sekoje nėra padding. Tai svarbu, kad dėmesio mechanizmas ignoruotų padding.\n",
        "\n",
        "-------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KIelvUwYebl"
      },
      "source": [
        "TransformerDecoder klasė:\n",
        "\n",
        "attention_1 = layers.MultiHeadAttention(...): Pirmas attention sluoksnys (Masked Multi-Head Attention). Jis veikia tik su target kalba, bet yra maskuotas, kad būtų išvengta \"žiūrėjimo į ateitį\" (causal masking).\n",
        "\n",
        "attention_2 = layers.MultiHeadAttention(...): Antras attention sluoksnys (Encoder-Decoder Attention). Jis veikia su target kalba (kaip užklausos) ir enkoderio išvestimi (kaip raktai ir vertės). Leidžia atkreipti dėmesį į šaltinio kalbos dalis.\n",
        "\n",
        "dense_proj = keras.Sequential(...): dense sluoksniai.\n",
        "\n",
        "layernorm_1, layernorm_2, layernorm_3: Normalizavimo sluoksniai.\n",
        "\n",
        "supports_masking = True: Nurodo, kad sluoksnis palaiko maskavimą.\n",
        "\n",
        "call(self, inputs, mask=None): Šis metodas apibrėžia, kaip duomenys apdorojami per dekoderio sluoksnį.\n",
        "\n",
        "inputs, encoder_outputs = inputs: priima iki šiol sugeneruotą kalbos seką (inputs) ir enkoderio išvestį (encoder_outputs).\n",
        "\n",
        "causal_mask = self.get_causal_attention_mask(inputs): Sukuriama causal mask, kad nežiūrėtų į ateitį.\n",
        "\n",
        "if mask is None: ... else: inputs_padding_mask, encoder_outputs_padding_mask = mask: Tvarkomos padding tiek tikslo kalbos sekai, tiek enkoderio išvesčiai.\n",
        "\n",
        "attention_output_1 = self.attention_1(...): Atliekamas pirmas casual attention skaičiavimas sekoje.\n",
        "\n",
        "out_1 = self.layernorm_1(inputs + attention_output_1): Prie sekos pridedama attention išvestis ir normalizuojama.\n",
        "\n",
        "attention_output_2 = self.attention_2(...): Atliekamas antras (enkoderis-dekoderis) attention skaičiavimas, naudojant out_1 ir encoder_outputs.\n",
        "\n",
        "out_2 = self.layernorm_2(out_1 + attention_output_2): Prie out_1 pridedama antro attention sluoksnio išvestis ir normalizuojama.\n",
        "\n",
        "proj_output = self.dense_proj(out_2): dense sluoksniai apdoroja normalizuotą išvestį.\n",
        "\n",
        "return self.layernorm_3(out_2 + proj_output): Prie out_2 pridedama dense sluoksnių išvestis ir atliekamas galutinis normalizavimas.\n",
        "\n",
        "get_causal_attention_mask(self, inputs): Šis metodas sukuria casual mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SR2EV0KbkfH9"
      },
      "outputs": [],
      "source": [
        "import keras.ops as ops\n",
        "\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(dense_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            padding_mask = ops.cast(mask[:, None, :], dtype=\"int32\")\n",
        "        else:\n",
        "            padding_mask = None\n",
        "\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "        )\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"embed_dim\": self.embed_dim,\n",
        "                \"dense_dim\": self.dense_dim,\n",
        "                \"num_heads\": self.num_heads,\n",
        "            }\n",
        "        )\n",
        "        return config\n",
        "\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = ops.shape(inputs)[-1]\n",
        "        positions = ops.arange(0, length, 1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return ops.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"sequence_length\": self.sequence_length,\n",
        "                \"vocab_size\": self.vocab_size,\n",
        "                \"embed_dim\": self.embed_dim,\n",
        "            }\n",
        "        )\n",
        "        return config\n",
        "\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(latent_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        inputs, encoder_outputs = inputs\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "\n",
        "        if mask is None:\n",
        "            inputs_padding_mask, encoder_outputs_padding_mask = None, None\n",
        "        else:\n",
        "            inputs_padding_mask, encoder_outputs_padding_mask = mask\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask,\n",
        "            query_mask=inputs_padding_mask,\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            query_mask=inputs_padding_mask,\n",
        "            key_mask=encoder_outputs_padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = ops.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = ops.arange(sequence_length)[:, None]\n",
        "        j = ops.arange(sequence_length)\n",
        "        mask = ops.cast(i >= j, dtype=\"int32\")\n",
        "        mask = ops.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = ops.concatenate(\n",
        "            [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])],\n",
        "            axis=0,\n",
        "        )\n",
        "        return ops.tile(mask, mult)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"embed_dim\": self.embed_dim,\n",
        "                \"latent_dim\": self.latent_dim,\n",
        "                \"num_heads\": self.num_heads,\n",
        "            }\n",
        "        )\n",
        "        return config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWOmn2xVkfH-"
      },
      "source": [
        "Paruošiame galutinį modelį"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xR05_vMRkfH-"
      },
      "outputs": [],
      "source": [
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)([x, encoder_outputs])\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "transformer = keras.Model(\n",
        "    {\"encoder_inputs\": encoder_inputs, \"decoder_inputs\": decoder_inputs},\n",
        "    decoder_outputs,\n",
        "    name=\"transformer\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz9Cl0MMkfH_"
      },
      "source": [
        "Treniruojame modelį"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "id": "1wn6-ovukfH_",
        "outputId": "72535da7-a279-4768-c966-dbe96c559a79"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"transformer\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ positional_embeddi… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">25,615,360</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbeddi…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ positional_embeddi… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">25,615,360</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbeddi…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encoder │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">9,453,568</span> │ positional_embed… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_decoder │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">17,856,000</span> │ positional_embed… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ transformer_enco… │\n",
              "│                     │                   │            │ not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                     │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ transformer_deco… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">25,650,000</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">50000</span>)            │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ positional_embeddi… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │ \u001b[38;5;34m25,615,360\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mPositionalEmbeddi…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ positional_embeddi… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │ \u001b[38;5;34m25,615,360\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mPositionalEmbeddi…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encoder │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │  \u001b[38;5;34m9,453,568\u001b[0m │ positional_embed… │\n",
              "│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_decoder │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │ \u001b[38;5;34m17,856,000\u001b[0m │ positional_embed… │\n",
              "│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ transformer_enco… │\n",
              "│                     │                   │            │ not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│                     │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ transformer_deco… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │ \u001b[38;5;34m25,650,000\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│                     │ \u001b[38;5;34m50000\u001b[0m)            │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,190,288</span> (397.45 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m104,190,288\u001b[0m (397.45 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,190,288</span> (397.45 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m104,190,288\u001b[0m (397.45 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m5205/5205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m741s\u001b[0m 137ms/step - accuracy: 0.0337 - loss: 6.6040 - val_accuracy: 0.0344 - val_loss: 6.1872\n",
            "Epoch 2/5\n",
            "\u001b[1m5205/5205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m719s\u001b[0m 138ms/step - accuracy: 0.0353 - loss: 6.1486 - val_accuracy: 0.0354 - val_loss: 6.1530\n",
            "Epoch 3/5\n",
            "\u001b[1m5205/5205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m722s\u001b[0m 134ms/step - accuracy: 0.0376 - loss: 6.0550 - val_accuracy: 0.0416 - val_loss: 5.9376\n",
            "Epoch 4/5\n",
            "\u001b[1m5205/5205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m699s\u001b[0m 134ms/step - accuracy: 0.0400 - loss: 5.9207 - val_accuracy: 0.0419 - val_loss: 5.8613\n",
            "Epoch 5/5\n",
            "\u001b[1m5205/5205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m716s\u001b[0m 138ms/step - accuracy: 0.0425 - loss: 5.8242 - val_accuracy: 0.0434 - val_loss: 5.8557\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7bf1e3ae13d0>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transformer.summary()\n",
        "transformer.compile(\n",
        "    keras.optimizers.AdamW(),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(ignore_class=0),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNzZI3WNkfH_"
      },
      "source": [
        "Paduodame sakinį vertimui, gauname rezultatą"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1CRaJT1kfIA",
        "outputId": "06a452dd-4e8b-4a15-f61b-73e94e83462e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Do you still think you'll be able to attend the meeting? | no no es\n",
            "It looks as though this summer will be cold again. | ella\n",
            "Wait one second. | el el\n",
            "I don't know what Tom knows, but Tom knows what I know. | tom no no\n",
            "He telephoned me again and again. | Él es es\n",
            "A magnet can pick up and hold many nails at a time. | Él un un\n",
            "You're going in the wrong direction. | Él es es\n",
            "Tom and Mary are still arguing. | tom no tom\n",
            "What's your favorite self-help book? | el el\n",
            "I decided to go to college to major in English. | no no no\n",
            "In my city, there is no school for learning Esperanto. | no no no\n",
            "Don't deceive him. | \n",
            "His whole family is like that. | ¿qué ¿qué ¿qué\n",
            "He used pigeons in his experiment. | Él un un\n",
            "Tom should definitely ask for Mary's opinion. | tom es es\n",
            "Do you like Mexican food? | es es es\n",
            "I only know that if I don't take this medicine every day, I'll die. | no no no\n",
            "It's in plain sight. | el el\n",
            "His novels are too deep for me. | el el\n",
            "This is quite tasty. | es es es\n",
            "Why didn't you tell me Tom was here? | no no no\n",
            "Which direction will he choose? | no no no\n",
            "He often eats fish for dinner. | Él es es\n",
            "Tom is just trying to be romantic, isn't he? | tom no tom\n",
            "He may have missed the plane. | Él es es\n",
            "He is a pioneer in this field. | Él un un\n",
            "This is a spotted dog. | Él es es\n",
            "The cottage reminded me of the happy times I had spent with her. | me me me\n",
            "There's a possibility that she'll succeed. | me me me\n",
            "I love my parents. | me me me\n"
          ]
        }
      ],
      "source": [
        "tgt_vocab = tgt_vectorization.get_vocabulary()\n",
        "tgt_index_lookup = dict(zip(range(len(tgt_vocab)), tgt_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = tgt_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer(\n",
        "            {\n",
        "                \"encoder_inputs\": tokenized_input_sentence,\n",
        "                \"decoder_inputs\": tokenized_target_sentence,\n",
        "            }\n",
        "        )\n",
        "        sampled_token_index = ops.convert_to_numpy(\n",
        "            ops.argmax(predictions[0, i, :])\n",
        "        ).item(0)\n",
        "        sampled_token = tgt_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(30):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    translated = decode_sequence(input_sentence)\n",
        "    translated = translated.replace(\"[PAD]\", \"\").replace(\"[start]\", \"\").replace(\"[end]\", \"\").strip()\n",
        "    print(f\"{input_sentence} | {translated}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPNiHXT7kfIA"
      },
      "source": [
        "Atliekame matavimus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blgkuVa5iQdi",
        "outputId": "e6f93d7b-556b-4e32-c1ee-cd4ca61dd28c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE-1 Score:  {'precision': <tf.Tensor: shape=(), dtype=float32, numpy=0.1833333522081375>, 'recall': <tf.Tensor: shape=(), dtype=float32, numpy=0.04875781759619713>, 'f1_score': <tf.Tensor: shape=(), dtype=float32, numpy=0.07424840331077576>}\n",
            "ROUGE-2 Score:  {'precision': <tf.Tensor: shape=(), dtype=float32, numpy=0.01666666753590107>, 'recall': <tf.Tensor: shape=(), dtype=float32, numpy=0.0025641026441007853>, 'f1_score': <tf.Tensor: shape=(), dtype=float32, numpy=0.004444444552063942>}\n",
            "Average BLEU Score:  2.4087026308951578e-233\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "rouge_1 = keras_hub.metrics.RougeN(order=1)\n",
        "rouge_2 = keras_hub.metrics.RougeN(order=2)\n",
        "\n",
        "bleu_scores = []\n",
        "\n",
        "for test_pair in test_pairs[:30]:\n",
        "    input_sentence = test_pair[0]\n",
        "    reference_sentence = test_pair[1]\n",
        "\n",
        "    translated_sentence = decode_sequence(input_sentence)\n",
        "    translated_sentence = translated_sentence.replace(\"[PAD]\", \"\").replace(\"[start]\", \"\").replace(\"[end]\", \"\").strip() # Use lowercase \"[start]\" and \"[end]\" based on the decode_sequence function\n",
        "\n",
        "    rouge_1.update_state(reference_sentence, translated_sentence)\n",
        "    rouge_2.update_state(reference_sentence, translated_sentence)\n",
        "\n",
        "    reference = [reference_sentence.split()]\n",
        "    candidate = translated_sentence.split()\n",
        "    bleu_scores.append(sentence_bleu(reference, candidate))\n",
        "\n",
        "print(\"ROUGE-1 Score: \", rouge_1.result())\n",
        "print(\"ROUGE-2 Score: \", rouge_2.result())\n",
        "\n",
        "print(\"Average BLEU Score: \", sum(bleu_scores) / len(bleu_scores))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
