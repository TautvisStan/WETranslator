{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEq4X2iJSm4r"
      },
      "source": [
        "Įdiegiame reikalingas bibliotekas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k33QJA29pUOS",
        "outputId": "c4324546-b971-4f19-9111-6144e41ce3cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.1/792.1 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "keras-nlp 0.18.1 requires keras-hub==0.18.1, but you have keras-hub 0.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "keras-nlp 0.18.1 requires keras-hub==0.18.1, but you have keras-hub 0.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade rouge-score\n",
        "!pip install -q --upgrade keras-hub\n",
        "!pip install -q --upgrade keras  # Upgrade to Keras 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g-e4kyYSpxs"
      },
      "source": [
        "Importuojame bibliotekas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yrER1mudpUOV"
      },
      "outputs": [],
      "source": [
        "import keras_hub\n",
        "import pathlib\n",
        "import random\n",
        "import numpy\n",
        "import keras\n",
        "from keras import ops\n",
        "\n",
        "import tensorflow.data as tf_data\n",
        "from tensorflow_text.tools.wordpiece_vocab import (\n",
        "    bert_vocab_from_dataset as bert_vocab,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJBlAj0EpUOW"
      },
      "source": [
        "Apsirašome hyperparametrus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4Wc0L7IpUOY"
      },
      "outputs": [],
      "source": [
        "ENG_VOCAB_SIZE = 40000\n",
        "TGT_VOCAB_SIZE = 40000\n",
        "MAX_SEQUENCE_LENGTH = 25\n",
        "EPOCHS = 10\n",
        "EMBED_DIM = 256\n",
        "INTERMEDIATE_DIM = 2048\n",
        "NUM_HEADS = 12\n",
        "BATCH_SIZE = 64\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arv5AitppUOZ"
      },
      "source": [
        "Duomenų failai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YOxTj_hSpUOa"
      },
      "outputs": [],
      "source": [
        "text_file = \"/content/spa.txt\"\n",
        "text_file_en = \"/content/QED.en-lt.en\"\n",
        "text_file_lt = \"/content/QED.en-lt.lt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyEFBl3RpUOb"
      },
      "source": [
        "Duomenų paruošimas. Viršuje kodas skirtas lietuviškam rinkiniui, apačioje - ispaniškui."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IXsMynpZpUOc"
      },
      "outputs": [],
      "source": [
        "# with open(text_file) as f:\n",
        "#     lines = f.read().split(\"\\n\")[:-1]\n",
        "# text_pairs = []\n",
        "# for line in lines:\n",
        "#     eng, spa = line.split(\"\\t\")\n",
        "#     eng = eng.lower()\n",
        "#     spa = spa.lower()\n",
        "#     text_pairs.append((eng, spa))\n",
        "\n",
        "with open(text_file_en) as f:\n",
        "    lines_en = f.read().split(\"\\n\")[:-1]\n",
        "with open(text_file_lt) as f:\n",
        "    lines_lt = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line_en, line_lt in zip(lines_en, lines_lt):\n",
        "    line_en = line_en.lower()\n",
        "    line_lt = line_lt.lower()\n",
        "    text_pairs.append((line_en, line_lt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8aX3nFZpUOc"
      },
      "source": [
        "Pažiūrime kelias poras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIdj3VwwpUOd",
        "outputId": "7f532f19-a20e-4d0a-94a1-58e19b6ffc34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('then i got a call from new york city asking if i could adapt these concepts to times square or the high line.', 'po to aš sulaukiau skambučio iš new york\\'o; manęs klausė, ar galėčiau pritaikyti savo kūrybą \"times\" aikštei ar \"high line\" parkui.')\n",
            "('how much money does it take to do this?', 'kiek reikia pinigų šiam reikalui?')\n",
            "(\"the man's business was a small one, and there was nothing in his house which could account for such elaborate preparations, and such an expenditure as they were at. it must, then, be something out of the house.\", 'žmogaus veikla buvo maža ir nebuvo nieko jo namuose, kurie galėtų sudaryti, pavyzdžiui parengti preparatai, ir tokios išlaidos, kaip jie buvo. , tada jis turi būti kažkas iš namo.')\n",
            "('when they were quite out of sight, phineas began to bestir himself.', 'kai jie buvo visiškai iš akių, phineas pradėjo sukrusti pats.')\n",
            "('let me concentrate, close my eyes. come, come.', 'leiskite man susikaupti, aš užsimerkiu.')\n"
          ]
        }
      ],
      "source": [
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5Eh4NxHpUOd"
      },
      "source": [
        "Išskiriame į treniravimo, validacijos ir testų poras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsJftNa2pUOd",
        "outputId": "ce69ef93-ae99-4ab5-d7ad-d536502b16bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "85435 total pairs\n",
            "59805 training pairs\n",
            "12815 validation pairs\n",
            "12815 test pairs\n"
          ]
        }
      ],
      "source": [
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2SSSQu6TRc9"
      },
      "source": [
        "Paruošiame tokenizer'ius su keras_hub biblioteka. Tokenizuoja \"sub-words\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Dm0z3lyjpUOe"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_word_piece(text_samples, vocab_size, reserved_tokens):\n",
        "    word_piece_ds = tf_data.Dataset.from_tensor_slices(text_samples)\n",
        "    vocab = keras_hub.tokenizers.compute_word_piece_vocabulary(\n",
        "        word_piece_ds.batch(1000).prefetch(2),\n",
        "        vocabulary_size=vocab_size,\n",
        "        reserved_tokens=reserved_tokens,\n",
        "    )\n",
        "    return vocab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1LuZKf2pUOe"
      },
      "source": [
        "Specialūs tokenai: [PAD], [UNK], [START], [END].\n",
        "Ištreniruojame žodynus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jn9vl68rpUOe"
      },
      "outputs": [],
      "source": [
        "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "\n",
        "eng_samples = [text_pair[0] for text_pair in train_pairs]\n",
        "eng_vocab = train_word_piece(eng_samples, ENG_VOCAB_SIZE, reserved_tokens)\n",
        "\n",
        "tgt_samples = [text_pair[1] for text_pair in train_pairs]\n",
        "tgt_vocab = train_word_piece(tgt_samples, TGT_VOCAB_SIZE, reserved_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXkuzPMzpUOf"
      },
      "source": [
        "Apsirašome tokenizerius su ištreniruotais žodynais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qPIvskqpUOf"
      },
      "outputs": [],
      "source": [
        "eng_tokenizer = keras_hub.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=eng_vocab, lowercase=False\n",
        ")\n",
        "tgt_tokenizer = keras_hub.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=tgt_vocab, lowercase=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07Kqm34nVULM"
      },
      "source": [
        "Suformatuojame duomenų rinkinius.\n",
        "Pridedame [PAD], bei [START] ir [END].\n",
        "\"make_dataset\" - sukuria TensorFlow Dataset objektą iš sąrašų."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jhsenOQpUOg"
      },
      "outputs": [],
      "source": [
        "\n",
        "def preprocess_batch(eng, tgt):\n",
        "    batch_size = ops.shape(tgt)[0]\n",
        "\n",
        "    eng = eng_tokenizer(eng)\n",
        "    tgt = tgt_tokenizer(tgt)\n",
        "\n",
        "    eng = eng[:, :MAX_SEQUENCE_LENGTH]\n",
        "    tgt = tgt[:, :MAX_SEQUENCE_LENGTH + 1]\n",
        "\n",
        "    eng_start_end_packer = keras_hub.layers.StartEndPacker(\n",
        "        sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "        pad_value=eng_tokenizer.token_to_id(\"[PAD]\"),\n",
        "    )\n",
        "    eng = eng_start_end_packer(eng)\n",
        "\n",
        "    tgt_start_end_packer = keras_hub.layers.StartEndPacker(\n",
        "        sequence_length=MAX_SEQUENCE_LENGTH + 1,\n",
        "        start_value=tgt_tokenizer.token_to_id(\"[START]\"),\n",
        "        end_value=tgt_tokenizer.token_to_id(\"[END]\"),\n",
        "        pad_value=tgt_tokenizer.token_to_id(\"[PAD]\"),\n",
        "    )\n",
        "    tgt = tgt_start_end_packer(tgt)\n",
        "\n",
        "    return (\n",
        "        {\n",
        "            \"encoder_inputs\": eng,\n",
        "            \"decoder_inputs\": tgt[:, :-1],\n",
        "        },\n",
        "        tgt[:, 1:],\n",
        "    )\n",
        "\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, tgt_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    tgt_texts = list(tgt_texts)\n",
        "    dataset = tf_data.Dataset.from_tensor_slices((eng_texts, tgt_texts))\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.map(preprocess_batch, num_parallel_calls=tf_data.AUTOTUNE)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSqbzlgupUOg"
      },
      "source": [
        "Pažiūrime į gautus shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_j9lgxzppUOg",
        "outputId": "77afa8ba-3f3b-4d3e-f5e2-5e6e170095e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (64, 25)\n",
            "inputs[\"decoder_inputs\"].shape: (64, 25)\n",
            "targets.shape: (64, 25)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhSykDyHpUOg"
      },
      "source": [
        "Paruošiamas modelis naudojantis keras_hub bibliotekas.\n",
        "\n",
        "Encoder:\n",
        "\n",
        "įvestis, embedding sluoksnis, encoder išvestis su attention.\n",
        "\n",
        "Decoder:\n",
        "\n",
        "įvestys, embedding sluoksnis, decoder sluoksnis su attention, dropout, dense išvestis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4w9SpAKpUOh"
      },
      "outputs": [],
      "source": [
        "# Encoder\n",
        "encoder_inputs = keras.Input(shape=(None,), name=\"encoder_inputs\")\n",
        "\n",
        "x = keras_hub.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=ENG_VOCAB_SIZE,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    embedding_dim=EMBED_DIM,\n",
        ")(encoder_inputs)\n",
        "\n",
        "encoder_outputs = keras_hub.layers.TransformerEncoder(\n",
        "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
        ")(inputs=x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = keras.Input(shape=(None,), name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, EMBED_DIM), name=\"decoder_state_inputs\")\n",
        "\n",
        "x = keras_hub.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=TGT_VOCAB_SIZE,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH + 1, # Changed here\n",
        "    embedding_dim=EMBED_DIM,\n",
        ")(decoder_inputs)\n",
        "\n",
        "x = keras_hub.layers.TransformerDecoder(\n",
        "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
        ")(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)\n",
        "x = keras.layers.Dropout(0.5)(x)\n",
        "decoder_outputs = keras.layers.Dense(TGT_VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "decoder = keras.Model(\n",
        "    [\n",
        "        decoder_inputs,\n",
        "        encoded_seq_inputs,\n",
        "    ],\n",
        "    decoder_outputs,\n",
        ")\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs],\n",
        "    decoder_outputs,\n",
        "    name=\"transformer\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw3QmNaOpUOh"
      },
      "source": [
        "Modelio treniravimas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 754
        },
        "id": "rf_PsCLApUOj",
        "outputId": "e3d7ea1c-f372-4716-f029-711396d0dac0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"transformer\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ token_and_position… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">10,246,400</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionE…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encoder │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,310,964</span> │ token_and_positi… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">22,097,192</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">40000</span>)            │            │ transformer_enco… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ token_and_position… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │ \u001b[38;5;34m10,246,400\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mTokenAndPositionE…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encoder │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m1,310,964\u001b[0m │ token_and_positi… │\n",
              "│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │ \u001b[38;5;34m22,097,192\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │ \u001b[38;5;34m40000\u001b[0m)            │            │ transformer_enco… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,654,556</span> (128.38 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m33,654,556\u001b[0m (128.38 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,654,556</span> (128.38 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m33,654,556\u001b[0m (128.38 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m935/935\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 103ms/step - accuracy: 0.4615 - loss: 4.5031 - val_accuracy: 0.5150 - val_loss: 3.4608\n",
            "Epoch 2/10\n",
            "\u001b[1m935/935\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 86ms/step - accuracy: 0.5211 - loss: 3.4285 - val_accuracy: 0.5370 - val_loss: 3.1623\n",
            "Epoch 3/10\n",
            "\u001b[1m935/935\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 85ms/step - accuracy: 0.5392 - loss: 3.1678 - val_accuracy: 0.5482 - val_loss: 3.0067\n",
            "Epoch 4/10\n",
            "\u001b[1m935/935\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 85ms/step - accuracy: 0.5517 - loss: 3.0025 - val_accuracy: 0.5575 - val_loss: 2.9244\n",
            "Epoch 5/10\n",
            "\u001b[1m935/935\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 85ms/step - accuracy: 0.5640 - loss: 2.8818 - val_accuracy: 0.5616 - val_loss: 2.9162\n",
            "Epoch 6/10\n",
            "\u001b[1m935/935\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 87ms/step - accuracy: 0.5751 - loss: 2.7855 - val_accuracy: 0.5685 - val_loss: 2.8665\n",
            "Epoch 7/10\n",
            "\u001b[1m935/935\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 85ms/step - accuracy: 0.5853 - loss: 2.7050 - val_accuracy: 0.5711 - val_loss: 2.8902\n",
            "Epoch 8/10\n",
            "\u001b[1m935/935\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 85ms/step - accuracy: 0.5944 - loss: 2.6343 - val_accuracy: 0.5744 - val_loss: 2.8735\n",
            "Epoch 9/10\n",
            "\u001b[1m935/935\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 87ms/step - accuracy: 0.6044 - loss: 2.5673 - val_accuracy: 0.5779 - val_loss: 2.8801\n",
            "Epoch 10/10\n",
            "\u001b[1m935/935\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 87ms/step - accuracy: 0.6138 - loss: 2.5050 - val_accuracy: 0.5816 - val_loss: 2.8991\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c429c21f690>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transformer.summary()\n",
        "transformer.compile(\n",
        "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "transformer.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJQNIxLipUOj"
      },
      "source": [
        "Paduodame sakinį vertimui, gauname rezultatą"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_Wbcfe0pUOj",
        "outputId": "b0907216-606d-40e3-8c37-aabcab9bb0bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "** Example 0 **\n",
            "mercutlo come, sir, your passado.\n",
            "mercutio ateik , pone , pone .\n",
            "\n",
            "** Example 1 **\n",
            "the truth may lie between these extremes.\n",
            "tiesa yra melaptimi tarp šių kraštutinis .\n",
            "\n",
            "** Example 2 **\n",
            "the reason for this is both that freedom is in and of itself good, valuable, worthwhile, essential to being human.\n",
            "dėl to , dėl to , kad tiek daug laiko ir vertinga , yra tokia svarbu , kad būtų svarbu būti svarbu būti\n",
            "\n",
            "** Example 3 **\n",
            "everybody would like to make people happier.\n",
            "visi žmonės , kaip žmonės daro laimingesni .\n",
            "\n",
            "** Example 4 **\n",
            "\"i knocked, but seemingly--\"\n",
            "\" aš pasirkas , bet aš \"\n",
            "\n",
            "** Example 5 **\n",
            "the first signal was for everybody to hold up these four-foot tall letters that spelled out \"look up more,\" the name of the project.\n",
            "pirmasis pasistogrįžtu , kad visi šie keturi atsirkas , kurie iš karto \" ra\n",
            "\n",
            "** Example 6 **\n",
            "and in many places they are worth less than goats and cows.\n",
            "ir daugelis yra tiek daug vietos , nei karbes ir karūrybims .\n",
            "\n",
            "** Example 7 **\n",
            "he is not supported by anyone\n",
            "jis nėra pagrįžtu , kas nors .\n",
            "\n",
            "** Example 8 **\n",
            "he's got pneumonia, and he looks like he needs intensive care.\n",
            "jis turi pianczen , ir atrodo , kaip jis rūpinosirbes .\n",
            "\n",
            "** Example 9 **\n",
            "come here!\n",
            "ateik čia !\n",
            "\n",
            "** Example 10 **\n",
            "this picture of seymour -- 25 years ago.\n",
            "ši nuotrauka yra eurovardykum -\n",
            "\n",
            "** Example 11 **\n",
            "and it contributes to about 70 samples of those thousand samples.\n",
            "ir tai buvo ir nuo 70 tūkstančių tūkstančių tūkstančių .\n",
            "\n",
            "** Example 12 **\n",
            "it is important for everyone\n",
            "svarbu visiems\n",
            "\n",
            "** Example 13 **\n",
            "so now, look at inner conditions.\n",
            "taigi , vidinio standartualutinis sąlygos .\n",
            "\n",
            "** Example 14 **\n",
            "'it was the best butter,' the march hare meekly replied.\n",
            "\" tai buvo geriausia , ponia kovo kiškis atsakė man atsakė , me .\n",
            "\n",
            "** Example 15 **\n",
            "two and a half weeks after the hemorrhage, the surgeons went in, and they removed a blood clot the size of a golf ball that was pushing on my language centers.\n",
            "dvi savaites ir po pusė po to , jis , jis nuėjo , ir chiruotę ir chirumb\n",
            "\n",
            "** Example 16 **\n",
            "and about this harpooneer, whom i have not yet seen, you persist in telling me the most mystifying and exasperating stories tending to beget in me an uncomfortable feeling towards the man whom you design for my bedfellow--a sort of connexion, landlord, which is an intimate and confidential one in the highest degree.\n",
            "ir apie tai , man atrodo , dar nepalgrana , jūs mane labiausiai mano labiausiai papro\n",
            "\n",
            "** Example 17 **\n",
            "shame is \"i am bad.\"\n",
            "gėda yra blogai \" . \"\n",
            "\n",
            "** Example 18 **\n",
            "'come on, then,' said the queen, 'and he shall tell you his history,'\n",
            "\" ateik , tada , \" , - sakė karalienė \" , jis jums , jis jums , \"\n",
            "\n",
            "** Example 19 **\n",
            "and therefore i will limit myself to only talk about the links between religion and sexuality.\n",
            "todėl aš priimčiau save susim apie iniciarbes ir inicia tarp kitko , ir\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def decode_sequences(input_sentences):\n",
        "    batch_size = 1\n",
        "\n",
        "    encoder_input_tokens = ops.convert_to_tensor(eng_tokenizer(input_sentences))\n",
        "    encoder_input_tokens = encoder_input_tokens[:, :MAX_SEQUENCE_LENGTH]\n",
        "    if len(encoder_input_tokens[0]) < MAX_SEQUENCE_LENGTH:\n",
        "        pads = ops.full((1, MAX_SEQUENCE_LENGTH - len(encoder_input_tokens[0])), 0)\n",
        "        encoder_input_tokens = ops.concatenate(\n",
        "            [encoder_input_tokens, pads], 1\n",
        "        )\n",
        "\n",
        "    def next(prompt, cache, index):\n",
        "\n",
        "        logits = transformer([encoder_input_tokens, prompt])[:, index - 1, :]\n",
        "        hidden_states = None\n",
        "        return logits, hidden_states, cache\n",
        "\n",
        "\n",
        "    length = MAX_SEQUENCE_LENGTH + 1\n",
        "    start = ops.full((batch_size, 1), tgt_tokenizer.token_to_id(\"[START]\"))\n",
        "    pad = ops.full((batch_size, length - 1), tgt_tokenizer.token_to_id(\"[PAD]\"))\n",
        "    prompt = ops.concatenate((start, pad), axis=-1)\n",
        "\n",
        "    generated_tokens = keras_hub.samplers.GreedySampler()(\n",
        "        next,\n",
        "        prompt,\n",
        "        stop_token_ids=[tgt_tokenizer.token_to_id(\"[END]\")],\n",
        "        index=1,  # Start sampling after start token.\n",
        "    )\n",
        "    generated_sentences = tgt_tokenizer.detokenize(generated_tokens)\n",
        "    return generated_sentences\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for i in range(20):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    translated = decode_sequences([input_sentence])[0]\n",
        "    translated = translated.replace(\"[PAD]\", \"\").replace(\"[START]\", \"\").replace(\"[END]\", \"\").strip()\n",
        "    print(f\"** Example {i} **\")\n",
        "    print(input_sentence)\n",
        "    print(translated)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q9PeYM9pUOk"
      },
      "source": [
        "Atliekame matavimus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwMEazRspUOk",
        "outputId": "79083b52-20b4-4c29-defa-adf00ca9b2e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE-1 Score:  {'precision': <tf.Tensor: shape=(), dtype=float32, numpy=0.28708896040916443>, 'recall': <tf.Tensor: shape=(), dtype=float32, numpy=0.2636148929595947>, 'f1_score': <tf.Tensor: shape=(), dtype=float32, numpy=0.2651635706424713>}\n",
            "ROUGE-2 Score:  {'precision': <tf.Tensor: shape=(), dtype=float32, numpy=0.10509468615055084>, 'recall': <tf.Tensor: shape=(), dtype=float32, numpy=0.10363973677158356>, 'f1_score': <tf.Tensor: shape=(), dtype=float32, numpy=0.10097070783376694>}\n",
            "Average BLEU Score:  7.617146588014677e-80\n"
          ]
        }
      ],
      "source": [
        "rouge_1 = keras_hub.metrics.RougeN(order=1)\n",
        "rouge_2 = keras_hub.metrics.RougeN(order=2)\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "bleu_scores = []\n",
        "\n",
        "for test_pair in test_pairs[:30]:\n",
        "    input_sentence = test_pair[0]\n",
        "    reference_sentence = test_pair[1]\n",
        "\n",
        "    translated_sentence = decode_sequences([input_sentence])[0]\n",
        "    translated_sentence = translated_sentence.replace(\"[PAD]\", \"\").replace(\"[START]\", \"\").replace(\"[END]\", \"\").strip()\n",
        "\n",
        "    rouge_1(reference_sentence, translated_sentence)\n",
        "    rouge_2(reference_sentence, translated_sentence)\n",
        "\n",
        "\n",
        "    reference = [reference_sentence.split()]\n",
        "    candidate = translated_sentence.split()\n",
        "    bleu_scores.append(sentence_bleu(reference, candidate))\n",
        "\n",
        "print(\"ROUGE-1 Score: \", rouge_1.result())\n",
        "print(\"ROUGE-2 Score: \", rouge_2.result())\n",
        "\n",
        "\n",
        "print(\"Average BLEU Score: \", sum(bleu_scores) / len(bleu_scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngw1BF2ptUwP"
      },
      "source": [
        "Išsaugome modelį kad galėtume naudoti vėliau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mHvfq2Gu3Ui"
      },
      "outputs": [],
      "source": [
        "# transformer.save(\"english_to_lt_translatorhub4.keras\")\n",
        "# with open(\"englt_vocab4.txt\", \"w\") as f:\n",
        "#         for token in eng_vocab:\n",
        "#             f.write(f\"{token}\\n\")\n",
        "\n",
        "# with open(\"ltu_vocab4.txt\", \"w\") as f:\n",
        "#         for token in tgt_vocab:\n",
        "#             f.write(f\"{token}\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
